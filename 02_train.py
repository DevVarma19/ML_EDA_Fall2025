# -*- coding: utf-8 -*-
"""02_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QGXxnHCZrmOLfLoduxNrC1bm4bdUeNPX
"""

from google.colab import drive
drive.mount('/content/drive')

import os, json, time
from datetime import datetime

# Root paths (match your 01_augment notebook)
PROJECT_ROOT = '/content/drive/MyDrive/EDA_Project'
DATA_ROOT    = f'{PROJECT_ROOT}/size_data_f1'
MODELS_DIR   = f'{PROJECT_ROOT}/models'
LOGS_DIR     = f'{PROJECT_ROOT}/logs'
CONFIGS_DIR  = f'{PROJECT_ROOT}/configs'

for p in [PROJECT_ROOT, DATA_ROOT, MODELS_DIR, LOGS_DIR, CONFIGS_DIR]:
    os.makedirs(p, exist_ok=True)

print("✓ Folders ready")
for p in [PROJECT_ROOT, DATA_ROOT, MODELS_DIR, LOGS_DIR]:
    print("→", p)

!pip -q install -U tensorflow scikit-learn

import tensorflow as tf, numpy as np
from sklearn.metrics import accuracy_score

print("TF:", tf.__version__)
print("GPU:", tf.config.list_physical_devices('GPU'))

"""### Config for SST-2"""

# What to run
dataset_name = 'sst2'
sizes   = ['1_tiny', '2_small', '3_standard', '4_full']   # you can slice this down while testing
methods = ['sr', 'ri', 'rd', 'rs']                        # choose subset if you want faster runs
alphas  = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]                 # choose subset if you want faster runs

# Model/data params
word2vec_len = 100    # you created word2vec.p with 100d GloVe in earlier steps
input_size   = 50     # SST-2 input length from a_config
num_classes  = 2

# Training params
batch_size   = 256
patience     = 3
max_epochs   = 100000    # rely on early stopping
val_split    = 0.1
shuffle      = True
verbose      = 1

# Paths
TEST_PATH = f"{DATA_ROOT}/test/{dataset_name}/test.txt"
assert os.path.exists(TEST_PATH), f"Missing test set at {TEST_PATH}"

MODEL_TYPE = "cnn"
# Logging
RUNS_CSV = f"{LOGS_DIR}/train_runs_{dataset_name}_{MODEL_TYPE}.csv"
if not os.path.exists(RUNS_CSV):
    with open(RUNS_CSV, "w") as f:
        f.write("timestamp,size,variant,method,alpha,model_path,test_acc,train_file,test_file,epochs,batch,val_split\n")

def now():
    return datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

"""## Utilities"""

import pickle, csv, os

def load_pickle(path):
    with open(path, 'rb') as f:
        return pickle.load(f)

def one_hot_to_categorical(y):
    return np.argmax(y, axis=1)

def get_x_y(txt_path, num_classes, word2vec_len, input_size, word2vec, percent_dataset=1.0):
    lines = open(txt_path, 'r', encoding='utf-8').read().splitlines()
    np.random.shuffle(lines)
    lines = lines[:int(percent_dataset * len(lines))]
    n = len(lines)
    X = np.zeros((n, input_size, word2vec_len), dtype=np.float32)
    Y = np.zeros((n, num_classes), dtype=np.float32)

    for i, line in enumerate(lines):
        parts = line.split('\t', 1)
        if len(parts) != 2:
            continue
        label = int(parts[0])
        sent = parts[1]
        words = sent.split()[:input_size]
        for j, w in enumerate(words):
            if w in word2vec:
                X[i, j, :] = word2vec[w]
        Y[i, label] = 1.0
    return X, Y

"""## Models"""

from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, Input, LSTM, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping

def build_cnn(sentence_length, word2vec_len, num_classes):
    model = Sequential([
        Conv1D(128, 5, activation='relu', input_shape=(sentence_length, word2vec_len)),
        GlobalMaxPooling1D(),
        Dense(20, activation='relu'),
        Dense(num_classes, activation='softmax'),
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def build_lstm(sentence_length, word2vec_len, num_classes):
    """
    Input: (batch, sentence_length, word2vec_len)
    Architecture: BiLSTM(64) -> Dropout -> BiLSTM(32) -> Dropout -> Dense -> Softmax
    """
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=(sentence_length, word2vec_len)),
        Dropout(0.5),
        Bidirectional(LSTM(32, return_sequences=False)),
        Dropout(0.5),
        Dense(20, activation='relu'),
        Dense(num_classes, activation='softmax'),
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def compute_metrics_from_probs(y_true_onehot, y_prob):
    y_true = np.argmax(y_true_onehot, axis=1)
    y_pred = np.argmax(y_prob, axis=1)
    acc = accuracy_score(y_true, y_pred)
    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)
    try:
        auroc = roc_auc_score(y_true, y_prob[:,1])
        prec, rec, _ = precision_recall_curve(y_true, y_prob[:,1])
        pr_auc = auc(rec, prec)
    except Exception:
        auroc, pr_auc = float('nan'), float('nan')
        prec, rec = np.array([np.nan]), np.array([np.nan])
    return {
        "acc": float(acc), "precision": float(p), "recall": float(r), "f1": float(f1),
        "auroc": float(auroc), "pr_auc": float(pr_auc),
        "pr_curve": (prec.tolist() if isinstance(prec, np.ndarray) else []),
        "rec_curve": (rec.tolist() if isinstance(rec, np.ndarray) else []),
    }

"""## Train/Eval helper that saves model + logs"""

import json, time
from tensorflow.keras.callbacks import EarlyStopping

def train_and_eval(size, variant_label, train_file, test_file, model_type="cnn", method=None, alpha=None):
    """
    variant_label: 'baseline' or 'eda'
    method/alpha:  filled when variant_label == 'eda'
    model_type: 'cnn' or 'lstm'
    """
    size_folder = f"{DATA_ROOT}/{size}/{dataset_name}"
    w2v_path    = f"{size_folder}/word2vec.p"
    print(w2v_path)
    assert os.path.exists(w2v_path), f"Missing word2vec pickle at {w2v_path}"
    assert os.path.exists(train_file), f"Missing train_file: {train_file}"
    assert os.path.exists(test_file),  f"Missing test_file:  {test_file}"

    # Load embeddings
    word2vec = load_pickle(w2v_path)

    # Load data
    X_train, Y_train = get_x_y(train_file, num_classes, word2vec_len, input_size, word2vec, percent_dataset=1.0)
    X_test,  Y_test  = get_x_y(test_file,  num_classes, word2vec_len, input_size, word2vec, percent_dataset=1.0)

    # Build model
    if model_type == "lstm":
        MODEL_TYPE = "lstm"
        model = build_lstm(input_size, word2vec_len, num_classes)
    else:
        MODEL_TYPE = "cnn"
        model = build_cnn(input_size, word2vec_len, num_classes)

    # Train
    t0 = time.time()
    cb = [EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)]
    hist = model.fit(
        X_train, Y_train,
        epochs=max_epochs,
        callbacks=cb,
        validation_split=val_split,
        batch_size=batch_size,   # for LSTM you can also try 128 if 256 is too big
        shuffle=shuffle,
        verbose=verbose
    )
    train_secs = round(time.time() - t0, 2)
    epochs_ran = len(hist.history['loss'])

    # Evaluate
    y_pred_prob = model.predict(X_test, verbose=0)
    metrics = compute_metrics_from_probs(Y_test, y_pred_prob)
    test_acc = accuracy_score(one_hot_to_categorical(Y_test), one_hot_to_categorical(y_pred_prob))

    # Save model with descriptive name
    parts = [
        dataset_name,
        f"size-{size}",
        model_type,
        variant_label if variant_label=='baseline' else f"eda-{method}_a{alpha}",
        now()
    ]
    model_fname = "__".join(parts) + ".keras"
    model_path  = f"{MODELS_DIR}/{model_fname}"
    model.save(model_path)

    # Save metadata JSON
    meta = {
        "dataset": dataset_name,
        "size": size,
        "variant": variant_label,
        "method": method,
        "alpha": alpha,
        "model_type": model_type,
        "input_size": input_size,
        "word2vec_len": word2vec_len,
        "epochs": epochs_ran,
        "batch_size": batch_size,
        "val_split": val_split,
        "train_seconds": train_secs,
        "train_file": train_file,
        "test_file": test_file,
        "test_acc": float(test_acc),
        "timestamp": now(),
    }
    meta_path = model_path.replace(".keras", ".json")
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)

    # Append to CSV log
    with open(RUNS_CSV, "a") as f:
        f.write(f"{now()},{size},{variant_label},{method},{alpha},{model_path},{test_acc},{train_file},{test_file},{epochs_ran},{batch_size},{val_split}\n")

    print(f"✓ Saved model: {model_path}")
    print(f"   Test Acc: {test_acc:.4f} | Trained in {train_secs}s, epochs: {epochs_ran}")
    return test_acc, model_path, meta_path

"""## Run CNN: Baseline + EDA"""

# Choose which sizes/methods/alphas to actually run now
run_sizes   = ['1_tiny','2_small','3_standard','4_full']              # try: ['1_tiny','2_small','3_standard','4_full']
run_methods = ['sr','ri','rd','rs']                                   # try: ['sr','ri','rd','rs']
run_alphas  = [0.1, 0.2, 0.3, 0.4, 0.5]                                          # try more later

results = []

for size in run_sizes:
    size_dir = f"{DATA_ROOT}/{size}/{dataset_name}"
    # Baseline
    train_file = f"{size_dir}/train_orig.txt"
    acc, mpath, jpath = train_and_eval(
        size=size,
        variant_label='baseline',
        train_file=train_file,
        test_file=TEST_PATH,
        model_type="cnn",
        method=None,
        alpha=None
    )
    results.append(("baseline", size, None, None, acc, mpath))

    # +EDA variants
    for method in run_methods:
        for alpha in run_alphas:
            train_file = f"{size_dir}/train_{method}_{alpha}.txt"
            if not os.path.exists(train_file):
                print("⚠️ Missing augmented file:", train_file)
                continue
            acc, mpath, jpath = train_and_eval(
                size=size,
                variant_label='eda',
                train_file=train_file,
                test_file=TEST_PATH,
                model_type="cnn",
                method=method,
                alpha=alpha
            )
            results.append(("eda", size, method, alpha, acc, mpath))

print("\nSummary:")
for r in results:
    print(r)

import pandas as pd
cols = ["variant","size","method","alpha","test_acc","model_path"]
df = pd.DataFrame(results, columns=cols).sort_values(by=["size","variant","method","alpha"])
df

"""## Run LSTM: baseline + EDA"""

# Focus run config for LSTM
run_sizes   = ['1_tiny','2_small','3_standard','4_full']              # try: ['1_tiny','2_small','3_standard','4_full']
run_methods = ['sr','ri','rd','rs']                                   # try: ['sr','ri','rd','rs']
run_alphas  = [0.1, 0.2, 0.3, 0.4, 0.5]

# LSTM usually benefits from a slightly smaller batch; uncomment to change
# batch_size = 128

lstm_results = []

for size in run_sizes:
    size_dir = f"{DATA_ROOT}/{size}/{dataset_name}"
    # Baseline
    train_file = f"{size_dir}/train_orig.txt"
    acc, mpath, jpath = train_and_eval(
        size=size,
        variant_label='baseline',
        train_file=train_file,
        test_file=TEST_PATH,
        model_type="lstm",
        method=None,
        alpha=None
    )
    lstm_results.append(("baseline", size, None, None, acc, mpath))

    # +EDA
    for method in run_methods:
        for alpha in run_alphas:
            train_file = f"{size_dir}/train_{method}_{alpha}.txt"
            if not os.path.exists(train_file):
                print("⚠️ Missing augmented file:", train_file)
                continue
            acc, mpath, jpath = train_and_eval(
                size=size,
                variant_label='eda',
                train_file=train_file,
                test_file=TEST_PATH,
                model_type="lstm",
                method=method,
                alpha=alpha
            )
            lstm_results.append(("eda", size, method, alpha, acc, mpath))

print("\nLSTM Summary:")
for r in lstm_results:
    print(r)

import glob
from tensorflow.keras.models import load_model

MODEL_DIR = "/content/drive/MyDrive/EDA_Project/models"

# Find the latest baseline and EDA model for LSTM 1_tiny
baseline_paths = glob.glob(f"{MODEL_DIR}/sst2__size-3_standard__lstm__baseline__*.keras")
eda_paths      = glob.glob(f"{MODEL_DIR}/sst2__size-3_standard__lstm__eda-ri_a0.3__*.keras")

print("Baseline candidates:", baseline_paths)
print("EDA candidates:", eda_paths)

"""## Evaluation Code"""

import os, json, glob
import numpy as np
import pandas as pd

MODELS_DIR = f"{PROJECT_ROOT}/models"
DATA_ROOT  = f"{PROJECT_ROOT}/size_data_f1"

def load_all_meta(models_dir=MODELS_DIR):
    rows = []
    for j in glob.glob(os.path.join(models_dir, "*.json")):
        try:
            meta = json.load(open(j))
            meta["meta_path"]  = j
            meta["model_path"] = j.replace(".json", ".keras")
            rows.append(meta)
        except Exception as e:
            print("Skip bad meta:", j, e)
    df = pd.DataFrame(rows)
    if df.empty:
        raise RuntimeError("No metadata JSONs found in models/")
    return df

def select_best_per_size(df, dataset="sst2", model_type="lstm"):
    sub = df[(df["dataset"]==dataset) & (df["model_type"]==model_type)]
    if sub.empty:
        raise RuntimeError(f"No entries for dataset={dataset}, model_type={model_type}")

    # best baseline per size
    base = (sub[sub["variant"]=="baseline"]
            .sort_values(["size","test_acc"], ascending=[True, False])
            .groupby("size", as_index=False).first())
    base = base.rename(columns={"model_path":"baseline_model_path",
                                "test_acc":"baseline_acc",
                                "meta_path":"baseline_meta_path"})

    # best EDA per size (across all methods/alphas)
    eda = (sub[sub["variant"]=="eda"]
           .sort_values(["size","test_acc"], ascending=[True, False])
           .groupby("size", as_index=False).first())
    eda = eda.rename(columns={"model_path":"eda_model_path",
                              "test_acc":"eda_acc",
                              "method":"eda_method",
                              "alpha":"eda_alpha",
                              "meta_path":"eda_meta_path"})

    merged = pd.merge(base[["size","baseline_model_path","baseline_acc","baseline_meta_path"]],
                      eda[["size","eda_model_path","eda_acc","eda_method","eda_alpha","eda_meta_path"]],
                      on="size", how="inner")

    # order sizes numerically if they look like 1_tiny/2_small/...
    size_order = {"1_tiny":1,"2_small":2,"3_standard":3,"4_full":4}
    merged["size_rank"] = merged["size"].map(size_order).fillna(999)
    merged = merged.sort_values("size_rank").drop(columns="size_rank").reset_index(drop=True)
    return merged

# Load all metadata once
all_meta_df = load_all_meta()
print("Loaded metadata entries:", len(all_meta_df))
# display(all_meta_df.head())

# Example selection (change to model_type="cnn" to switch)
best_by_size_lstm = select_best_per_size(all_meta_df, dataset="sst2", model_type="lstm")
best_by_size_cnn  = select_best_per_size(all_meta_df, dataset="sst2", model_type="cnn")
display(best_by_size_lstm)
best_by_size_lstm.to_csv(f"best_lstm_by_size.csv", index=False)
display(best_by_size_cnn)

from tensorflow.keras.models import load_model
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt

def load_test(size="1_tiny", dataset="sst2", num_classes=2, word2vec_len=100, input_size=50):
    w2v = load_pickle(f"{DATA_ROOT}/{size}/{dataset}/word2vec.p")
    X_t, Y_t = get_x_y(TEST_PATH, num_classes, word2vec_len, input_size, w2v)
    y_true = one_hot_to_categorical(Y_t)
    return X_t, y_true

def metrics_from_preds(y_true, y_pred):
    acc = accuracy_score(y_true, y_pred)
    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary", zero_division=0)
    return acc, p, r, f1

def eval_model(model_path, size, dataset="sst2"):
    X_t, y_true = load_test(size=size, dataset=dataset)
    model = load_model(model_path)
    y_prob = model.predict(X_t, verbose=0)
    y_pred = np.argmax(y_prob, axis=1)
    return y_true, y_pred

def plot_cm(cm, title):
    fig, ax = plt.subplots(figsize=(5,4))
    im = ax.imshow(cm, cmap="Blues")
    for (i, j), v in np.ndenumerate(cm):
        ax.text(j, i, f"{v}", ha='center', va='center', color="black")
    ax.set_title(title)
    ax.set_xlabel("Predicted label"); ax.set_ylabel("True label")
    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    plt.show()

def analyze_selection(best_df, model_type_label="lstm"):
    rows = []
    for _, row in best_df.iterrows():
        size = row["size"]
        base_path = row["baseline_model_path"]
        eda_path  = row["eda_model_path"]
        eda_method, eda_alpha = row["eda_method"], row["eda_alpha"]

        # Baseline
        y_true, y_pred_base = eval_model(base_path, size)
        acc_b, p_b, r_b, f1_b = metrics_from_preds(y_true, y_pred_base)
        cm_b = confusion_matrix(y_true, y_pred_base)

        # EDA
        y_true_eda, y_pred_eda = eval_model(eda_path, size)
        acc_e, p_e, r_e, f1_e = metrics_from_preds(y_true_eda, y_pred_eda)
        cm_e = confusion_matrix(y_true_eda, y_pred_eda)

        print(f"\n=== {model_type_label.upper()} | Size: {size} ===")
        print(f"Baseline: acc={acc_b:.3f}  P={p_b:.3f}  R={r_b:.3f}  F1={f1_b:.3f}")
        print(f"Best EDA ({eda_method}, α={eda_alpha}): acc={acc_e:.3f}  P={p_e:.3f}  R={r_e:.3f}  F1={f1_e:.3f}")

        plot_cm(cm_b, f"{model_type_label.upper()} {size} — Baseline")
        plot_cm(cm_e, f"{model_type_label.upper()} {size} — EDA ({eda_method}, α={eda_alpha})")

        rows.append({
            "model": model_type_label, "size": size,
            "baseline_acc": acc_b, "baseline_P": p_b, "baseline_R": r_b, "baseline_F1": f1_b,
            "eda_acc": acc_e, "eda_P": p_e, "eda_R": r_e, "eda_F1": f1_e,
            "eda_method": eda_method, "eda_alpha": eda_alpha,
            "baseline_model_path": base_path, "eda_model_path": eda_path
        })
    return pd.DataFrame(rows)

# Analyze LSTM best per size
lstm_summary = analyze_selection(best_by_size_lstm[:], model_type_label="lstm")
display(lstm_summary.sort_values(["size"]))

print("----------------------------")
# Analyze CNN best per size
cnn_summary = analyze_selection(best_by_size_cnn[:], model_type_label="cnn")
display(cnn_summary.sort_values(["size"]))

"""## Visualizations"""

# Commented out IPython magic to ensure Python compatibility.
# -*- coding: utf-8 -*-
"""
Colab-ready: Summarize EDA experiments from /models JSONs and generate tables/plots.

Reads ONLY from JSON files in MODELS_DIR. Works with:
- "legacy" JSONs (with test_acc but no test_metrics/paths)
- "new" JSONs (with test_metrics dict and paths)

Outputs:
- tables/summary_all_runs.csv                (every run)
- tables/summary_best_per_size__<model>.csv  (baseline vs best EDA per size)
- tables/paper_style_table.csv               (paper-like compact table)
- figures/acc_vs_pct__cnn.png
- figures/acc_vs_pct__lstm.png
- figures/acc_vs_pct__overlay.png
"""

# ---------- COLAB SETUP ----------
import os, glob, json, sys
from collections import defaultdict

try:
    # Mount Google Drive if we're in Colab
    import google.colab  # type: ignore
    from google.colab import drive  # type: ignore
    if not os.path.exists("/content/drive"):
        drive.mount('/content/drive')
    IN_COLAB = True
except Exception:
    IN_COLAB = False

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Matplotlib inline for Colab/Jupyter
# %matplotlib inline

# ---------- CONFIG ----------
# If you set PROJECT_ROOT as an env var, that wins. Otherwise we try Drive, then /content.
if "PROJECT_ROOT" in os.environ:
    PROJECT_ROOT = os.environ["PROJECT_ROOT"]
else:
    default_root = "/content/drive/MyDrive/EDA_Project" if IN_COLAB else "/content/EDA_Project"
    PROJECT_ROOT = default_root

MODELS_DIR   = os.path.join(PROJECT_ROOT, "models_v1")
OUT_FIG      = os.path.join(PROJECT_ROOT, "figures_v1")
OUT_TAB      = os.path.join(PROJECT_ROOT, "tables_v1")
os.makedirs(OUT_FIG, exist_ok=True)
os.makedirs(OUT_TAB, exist_ok=True)

# Adjust if your naming differs
TARGET_DATASET = "sst2"
SIZE_TO_PCT = {"1_tiny":5, "2_small":20, "3_standard":50, "4_full":100}

print(f"PROJECT_ROOT: {PROJECT_ROOT}")
print(f"MODELS_DIR:   {MODELS_DIR}")
print(f"Writing to:   figures/ -> {OUT_FIG}\n              tables/  -> {OUT_TAB}")

# ---------- LOADING ----------
def load_all_meta(models_dir=MODELS_DIR):
    """
    Load every *.json in models_dir.
    Normalize fields so downstream code can rely on a flat 'acc' column.
    """
    if not os.path.isdir(models_dir):
        raise RuntimeError(f"Models directory not found: {models_dir}")

    rows = []
    paths = sorted(glob.glob(os.path.join(models_dir, "*.json")))
    if not paths:
        raise RuntimeError(f"No JSON metadata found in {models_dir}")

    for jp in paths:
        try:
            with open(jp, "r", encoding="utf-8") as f:
                m = json.load(f)
            m["meta_path"] = jp

            # Flatten accuracy and optional metrics:
            if isinstance(m.get("test_metrics"), dict):
                tm = m["test_metrics"]
                m["acc"]       = tm.get("acc", np.nan)
                m["precision"] = tm.get("precision", np.nan)
                m["recall"]    = tm.get("recall", np.nan)
                m["f1"]        = tm.get("f1", np.nan)
                m["auroc"]     = tm.get("auroc", np.nan)
                m["pr_auc"]    = tm.get("pr_auc", np.nan)
            else:
                # legacy: only test_acc stored
                m["acc"]       = m.get("test_acc", np.nan)
                m["precision"] = np.nan
                m["recall"]    = np.nan
                m["f1"]        = np.nan
                m["auroc"]     = np.nan
                m["pr_auc"]    = np.nan

            rows.append(m)
        except Exception as e:
            print(f"Skip bad JSON: {jp}  ({e})")
    df = pd.DataFrame(rows)
    if df.empty:
        raise RuntimeError(f"No usable JSON metadata found in {models_dir}")
    return df

try:
    df = load_all_meta()
    print("Loaded JSONs:", len(df))
except Exception as e:
    print(f"[ERROR] {e}")
    raise

# ---------- FILTER / NORMALIZE ----------
if "dataset" not in df.columns:
    print("[WARN] 'dataset' column missing in JSONs; proceeding without dataset filter.")
    df_target = df.copy()
else:
    df_target = df[df["dataset"] == TARGET_DATASET].copy()
    if df_target.empty:
        print(f"[WARN] No rows for dataset == '{TARGET_DATASET}'. Falling back to all datasets.")
        df_target = df.copy()

# pct mapping: from size label via SIZE_TO_PCT, or pass-through if pct present, else NaN
if "size" not in df_target.columns:
    df_target["size"] = None
df_target["pct"] = np.where(
    df_target.get("pct").notnull() if "pct" in df_target.columns else False,
    df_target.get("pct", np.nan),
    df_target["size"].map(SIZE_TO_PCT)
)

# Safe sort keys if missing
for col in ["model_type","variant","method","alpha","timestamp"]:
    if col not in df_target.columns:
        df_target[col] = None

df_target = df_target.sort_values(
    ["model_type","pct","variant","method","alpha","timestamp"],
    na_position="last"
).reset_index(drop=True)

# Save raw table of ALL runs
all_runs_csv = os.path.join(OUT_TAB, "summary_all_runs.csv")
df_target.to_csv(all_runs_csv, index=False)
print("Saved:", all_runs_csv)

# ---------- HELPERS ----------
def best_per_size(df_in, model_type="cnn"):
    """
    For a given model_type, return a table with:
      size, pct, baseline_acc, eda_acc, eda_method, eda_alpha
    Selects the single best baseline and best EDA by 'acc' within each size.
    If metrics are missing (legacy), uses 'acc' only.
    """
    if "model_type" not in df_in.columns or "variant" not in df_in.columns:
        return pd.DataFrame(columns=["model_type","size","pct","baseline_acc","eda_acc","eda_method","eda_alpha"])

    sub = df_in[(df_in["model_type"]==model_type)].copy()
    out = []
    for size, grp in sub.groupby("size", dropna=False):
        pct = SIZE_TO_PCT.get(size, np.nan) if pd.notnull(size) else grp["pct"].dropna().unique()
        if isinstance(pct, np.ndarray) and len(pct)==1:
            pct = float(pct[0])
        elif isinstance(pct, np.ndarray) and len(pct)==0:
            pct = np.nan

        base = grp[grp["variant"]=="baseline"].sort_values("acc", ascending=False)
        eda  = grp[grp["variant"]=="eda"].sort_values("acc", ascending=False)
        if base.empty or eda.empty:
            continue
        b = base.iloc[0]
        e = eda.iloc[0]
        out.append({
            "model_type": model_type,
            "size": size,
            "pct": pct,
            "baseline_acc": float(b["acc"]) if pd.notnull(b["acc"]) else np.nan,
            "eda_acc": float(e["acc"]) if pd.notnull(e["acc"]) else np.nan,
            "eda_method": e.get("method", None),
            "eda_alpha": e.get("alpha", None)
        })
    if not out:
        return pd.DataFrame(columns=["model_type","size","pct","baseline_acc","eda_acc","eda_method","eda_alpha"])
    return pd.DataFrame(out).sort_values("pct").reset_index(drop=True)

cnn_best  = best_per_size(df_target, "cnn")
lstm_best = best_per_size(df_target, "lstm")

cnn_csv  = os.path.join(OUT_TAB, "summary_best_per_size__cnn.csv")
lstm_csv = os.path.join(OUT_TAB, "summary_best_per_size__lstm.csv")
cnn_best.to_csv(cnn_csv, index=False)
lstm_best.to_csv(lstm_csv, index=False)
print("Saved:", cnn_csv)
print("Saved:", lstm_csv)

# ---------- PLOTS ----------
def plot_acc_vs_pct(summary_df, title, out_png):
    if summary_df.empty:
        print(f"[warn] nothing to plot for {title}")
        return
    x   = summary_df["pct"].values
    y_n = summary_df["baseline_acc"].values
    y_e = summary_df["eda_acc"].values

    plt.figure(figsize=(6,4))
    plt.plot(x, y_n, marker='o', linestyle='-', label='Normal')
    plt.plot(x, y_e, marker='^', linestyle='-', label='EDA')
    plt.title(title)
    plt.xlabel("% of training data")
    plt.ylabel("Accuracy")
    plt.ylim(0.4, 1.0)
    plt.grid(True, alpha=0.3)
    try:
        plt.xticks(sorted([int(v) for v in pd.Series(x).dropna().unique()]))
    except Exception:
        pass
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_png, dpi=180)
    plt.close()
    print("Saved figure:", out_png)

plot_acc_vs_pct(cnn_best,  "SST-2 (CNN) — Accuracy vs % Dataset",  os.path.join(OUT_FIG, "acc_vs_pct__cnn.png"))
plot_acc_vs_pct(lstm_best, "SST-2 (LSTM) — Accuracy vs % Dataset", os.path.join(OUT_FIG, "acc_vs_pct__lstm.png"))

# Overlay
def overlay_plot(cnn_df, lstm_df, out_png):
    if cnn_df.empty and lstm_df.empty:
        print("[warn] nothing to plot in overlay")
        return
    plt.figure(figsize=(7,4))
    if not cnn_df.empty:
        plt.plot(cnn_df["pct"],  cnn_df["baseline_acc"], 'o-', label='CNN Normal')
        plt.plot(cnn_df["pct"],  cnn_df["eda_acc"],      '^-', label='CNN EDA')
    if not lstm_df.empty:
        plt.plot(lstm_df["pct"], lstm_df["baseline_acc"], 'o--', label='LSTM Normal')
        plt.plot(lstm_df["pct"], lstm_df["eda_acc"],      '^--', label='LSTM EDA')
    plt.title("SST-2 — Accuracy vs % Dataset (CNN & LSTM)")
    plt.xlabel("% of training data")
    plt.ylabel("Accuracy")
    plt.ylim(0.4, 1.0)
    plt.grid(True, alpha=0.3)
    ticks = sorted(set(pd.Series(cnn_df["pct"]).dropna().tolist()).union(set(pd.Series(lstm_df["pct"]).dropna().tolist())))
    if ticks:
        try:
            plt.xticks([int(v) for v in ticks])
        except Exception:
            plt.xticks(ticks)
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_png, dpi=180)
    plt.close()
    print("Saved figure:", out_png)

overlay_plot(cnn_best, lstm_best, os.path.join(OUT_FIG, "acc_vs_pct__overlay.png"))

# ---------- PAPER-STYLE TABLE ----------
def paper_style_table(cnn_df, lstm_df):
    """
    Build a compact table like the paper:
    Rows: LSTM, +EDA, CNN, +EDA, Average, +EDA
    Cols: 5%, 20%, 50%, 100%
    """
    order = [5, 20, 50, 100]
    def extract_row(df, col):
        m = {p:a for p,a in zip(df["pct"], df[col])}
        return [m.get(p, np.nan) for p in order]

    rnn_norm = extract_row(lstm_df, "baseline_acc") if not lstm_df.empty else [np.nan]*4
    rnn_eda  = extract_row(lstm_df, "eda_acc")      if not lstm_df.empty else [np.nan]*4
    cnn_norm = extract_row(cnn_df,  "baseline_acc") if not cnn_df.empty else [np.nan]*4
    cnn_eda  = extract_row(cnn_df,  "eda_acc")      if not cnn_df.empty else [np.nan]*4

    table = pd.DataFrame({
        "Model": ["LSTM"," +EDA","CNN"," +EDA","Average"," +EDA"],
        "5%":   [rnn_norm[0], rnn_eda[0], cnn_norm[0], cnn_eda[0],
                 np.nanmean([rnn_norm[0], cnn_norm[0]]), np.nanmean([rnn_eda[0], cnn_eda[0]])],
        "20%":  [rnn_norm[1], rnn_eda[1], cnn_norm[1], cnn_eda[1],
                 np.nanmean([rnn_norm[1], cnn_norm[1]]), np.nanmean([rnn_eda[1], cnn_eda[1]])],
        "50%":  [rnn_norm[2], rnn_eda[2], cnn_norm[2], cnn_eda[2],
                 np.nanmean([rnn_norm[2], cnn_norm[2]]), np.nanmean([rnn_eda[2], cnn_eda[2]])],
        "100%": [rnn_norm[3], rnn_eda[3], cnn_norm[3], cnn_eda[3],
                 np.nanmean([rnn_norm[3], cnn_norm[3]]), np.nanmean([rnn_eda[3], cnn_eda[3]])],
    })
    return table

paper_tbl = paper_style_table(cnn_best, lstm_best)
paper_csv = os.path.join(OUT_TAB, "paper_style_table.csv")
paper_tbl.to_csv(paper_csv, index=False)
print("Saved:", paper_csv)

# Also print a pretty version rounded to 1 decimal %
def fmt_pct(x):
    try:
        return f"{100*float(x):.1f}%"
    except Exception:
        return ""

pretty = paper_tbl.copy()
for c in ["5%","20%","50%","100%"]:
    pretty[c] = pretty[c].apply(fmt_pct)

print("\nPaper-style table:\n", pretty.to_string(index=False))

# Optional: display the saved CSV paths for convenience
print("\nArtifacts:")
print(" -", all_runs_csv)
print(" -", cnn_csv)
print(" -", lstm_csv)
print(" -", paper_csv)
print(" -", os.path.join(OUT_FIG, "acc_vs_pct__cnn.png"))
print(" -", os.path.join(OUT_FIG, "acc_vs_pct__lstm.png"))
print(" -", os.path.join(OUT_FIG, "acc_vs_pct__overlay.png"))

