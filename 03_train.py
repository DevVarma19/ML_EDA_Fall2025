# -*- coding: utf-8 -*-
"""03_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u7hDNdc0dxjDfhB6U4oUTGzA6pPfnMCb
"""

from google.colab import drive
drive.flush_and_unmount()

from google.colab import drive
drive.mount('/content/drive')

"""## Imports, paths, seeds"""

# --- imports
import os, json, time, glob, pickle, random
from datetime import datetime

import numpy as np
import tensorflow as tf

from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, confusion_matrix,
    roc_auc_score, roc_curve, precision_recall_curve, auc
)

from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# --- ensure these exist in your notebook/session
try:
    PROJECT_ROOT
except NameError:
    PROJECT_ROOT = '/content/drive/MyDrive/EDA_Project'
DATA_ROOT   = f'{PROJECT_ROOT}/size_data_f1'
MODELS_DIR  = f'{PROJECT_ROOT}/models'
LOGS_DIR    = f'{PROJECT_ROOT}/logs'
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOGS_DIR, exist_ok=True)

# --- dataset config (SST-2)
dataset_name = 'sst2'
TEST_PATH = f"{DATA_ROOT}/test/{dataset_name}/test.txt"

# --- reproducibility
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)

# --- defaults (match your earlier constants)
word2vec_len = 100
input_size   = 50
num_classes  = 2
batch_size   = 256   # 128 if LSTM is tight
patience     = 3
max_epochs   = 100000
val_split    = 0.1
shuffle      = True
verbose      = 1

def now() -> str:
    return datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

"""## Utilities"""

def load_pickle(path):
    with open(path, 'rb') as f:
        return pickle.load(f)

def one_hot_to_categorical(y):
    return np.argmax(y, axis=1)

def get_x_y(txt_path, num_classes, word2vec_len, input_size, word2vec, percent_dataset=1.0):
    lines = open(txt_path, 'r', encoding='utf-8').read().splitlines()
    np.random.shuffle(lines)
    lines = lines[:int(percent_dataset * len(lines))]
    n = len(lines)
    X = np.zeros((n, input_size, word2vec_len), dtype=np.float32)
    Y = np.zeros((n, num_classes), dtype=np.float32)
    for i, line in enumerate(lines):
        parts = line.split('\t', 1)
        if len(parts) != 2:
            continue
        label = int(parts[0])
        sent  = parts[1]
        words = sent.split()[:input_size]
        for j, w in enumerate(words):
            if w in word2vec:
                X[i, j, :] = word2vec[w]
        Y[i, label] = 1.0
    return X, Y

"""## Models"""

def build_cnn(sentence_length, word2vec_len, num_classes):
    model = Sequential([
        Conv1D(128, 5, activation='relu', input_shape=(sentence_length, word2vec_len)),
        GlobalMaxPooling1D(),
        Dense(20, activation='relu'),
        Dense(num_classes, activation='softmax'),
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def build_lstm(sentence_length, word2vec_len, num_classes):
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=(sentence_length, word2vec_len)),
        Dropout(0.5),
        Bidirectional(LSTM(32, return_sequences=False)),
        Dropout(0.5),
        Dense(20, activation='relu'),
        Dense(num_classes, activation='softmax'),
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

"""## Metrics: full set (Acc, P/R/F1, AUROC, CM, ROC/PR curves)"""

def compute_all_metrics(y_true_onehot, y_prob):
    """
    Returns a dict of metrics PLUS raw curve arrays.
    - accuracy, precision, recall, f1, auroc
    - confusion_matrix (2x2)
    - roc_curve: fpr, tpr, thresholds
    - pr_curve: precision, recall, thresholds
    """
    y_true = np.argmax(y_true_onehot, axis=1)
    y_pred = np.argmax(y_prob, axis=1)

    # core metrics
    acc = accuracy_score(y_true, y_pred)
    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)

    # probabilistic curves & scores
    try:
        pos_prob = y_prob[:, 1]
        auroc = roc_auc_score(y_true, pos_prob)
        fpr, tpr, roc_th = roc_curve(y_true, pos_prob)
        pr_prec, pr_rec, pr_th = precision_recall_curve(y_true, pos_prob)
        pr_auc = auc(pr_rec, pr_prec)
    except Exception:
        auroc, pr_auc = float('nan'), float('nan')
        fpr = tpr = roc_th = pr_prec = pr_rec = pr_th = np.array([])

    cm = confusion_matrix(y_true, y_pred)

    return {
        "acc": float(acc),
        "precision": float(p),
        "recall": float(r),
        "f1": float(f1),
        "auroc": float(auroc),
        "pr_auc": float(pr_auc),
        "cm": cm,
        "roc_curve": {"fpr": fpr, "tpr": tpr, "thresholds": roc_th},
        "pr_curve":  {"precision": pr_prec, "recall": pr_rec, "thresholds": pr_th},
    }

"""## Updated training: saves ALL metrics & curves"""

def train_and_eval(
    size, variant_label, train_file, test_file,
    model_type="cnn", method=None, alpha=None,
    input_size_=input_size, word2vec_len_=word2vec_len,
    batch_size_=batch_size, patience_=patience, max_epochs_=max_epochs,
    val_split_=val_split, shuffle_=shuffle, verbose_=verbose, seed=SEED
):
    # paths
    size_folder = f"{DATA_ROOT}/{size}/{dataset_name}"
    w2v_path    = f"{size_folder}/word2vec.p"
    assert os.path.exists(w2v_path), f"Missing word2vec pickle at {w2v_path}"
    assert os.path.exists(train_file), f"Missing train_file: {train_file}"
    assert os.path.exists(test_file),  f"Missing test_file:  {test_file}"

    # embeddings
    word2vec = load_pickle(w2v_path)

    # data
    X_train, Y_train = get_x_y(train_file, num_classes, word2vec_len_, input_size_, word2vec, percent_dataset=1.0)
    X_test,  Y_test  = get_x_y(test_file,  num_classes, word2vec_len_, input_size_, word2vec, percent_dataset=1.0)

    # model
    model = build_lstm(input_size_, word2vec_len_, num_classes) if model_type=="lstm" else build_cnn(input_size_, word2vec_len_, num_classes)

    # names
    tag_variant = variant_label if variant_label=='baseline' else f"eda-{method}_a{alpha}"
    stub = "__".join([dataset_name, f"size-{size}", model_type, tag_variant, now()])
    model_path  = f"{MODELS_DIR}/{stub}.keras"
    ckpt_path   = f"{MODELS_DIR}/{stub}.best.keras"
    preds_path  = f"{MODELS_DIR}/{stub}.preds.npz"
    hist_path   = f"{MODELS_DIR}/{stub}.history.json"
    cm_path     = f"{MODELS_DIR}/{stub}.cm.json"
    curves_path = f"{MODELS_DIR}/{stub}.curves.npz"
    meta_path   = f"{MODELS_DIR}/{stub}.json"

    # callbacks
    cbs = [
        ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True, verbose=0),
        EarlyStopping(monitor='val_loss', patience=patience_, restore_best_weights=True)
    ]

    # train
    t0 = time.time()
    hist = model.fit(
        X_train, Y_train,
        epochs=max_epochs_,
        callbacks=cbs,
        validation_split=val_split_,
        batch_size=batch_size_,
        shuffle=shuffle_,
        verbose=verbose_
    )
    train_secs = round(time.time() - t0, 2)
    epochs_ran = len(hist.history['loss'])

    # evaluate on TEST
    y_prob = model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_prob, axis=1)
    test_metrics = compute_all_metrics(Y_test, y_prob)

    # save artifacts
    model.save(model_path)
    with open(hist_path, "w") as f:
        json.dump({k: list(map(float, v)) for k,v in hist.history.items()}, f, indent=2)
    np.savez_compressed(preds_path, y_true=np.argmax(Y_test,axis=1), y_prob=y_prob, y_pred=y_pred)
    with open(cm_path, "w") as f:
        json.dump({"confusion_matrix": test_metrics["cm"].tolist()}, f, indent=2)
    np.savez_compressed(
        curves_path,
        roc_fpr=test_metrics["roc_curve"]["fpr"],
        roc_tpr=test_metrics["roc_curve"]["tpr"],
        roc_th=test_metrics["roc_curve"]["thresholds"],
        pr_precision=test_metrics["pr_curve"]["precision"],
        pr_recall=test_metrics["pr_curve"]["recall"],
        pr_th=test_metrics["pr_curve"]["thresholds"]
    )

    meta = {
        "dataset": dataset_name,
        "size": size,
        "variant": variant_label,
        "method": method,
        "alpha": alpha,
        "model_type": model_type,
        "seed": seed,
        "input_size": int(input_size_),
        "word2vec_len": int(word2vec_len_),
        "epochs": int(epochs_ran),
        "batch_size": int(batch_size_),
        "val_split": float(val_split_),
        "train_seconds": float(train_secs),
        "train_file": train_file,
        "test_file": test_file,
        "w2v_pickle": w2v_path,
        "test_metrics": {k: (float(v) if isinstance(v, (np.floating,)) else v)
                         for k,v in test_metrics.items() if k not in ["cm","roc_curve","pr_curve"]},
        "timestamp": now(),
        "paths": {
            "model": model_path, "checkpoint": ckpt_path,
            "history": hist_path, "preds": preds_path, "cm": cm_path, "curves": curves_path
        }
    }
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)

    # log summary line
    print(f"✓ {model_type.upper()} {size} {tag_variant} | "
          f"Acc={test_metrics['acc']:.3f} P={test_metrics['precision']:.3f} R={test_metrics['recall']:.3f} "
          f"F1={test_metrics['f1']:.3f} AUROC={test_metrics['auroc']:.3f} PRAUC={test_metrics['pr_auc']:.3f}")
    return test_metrics, model_path, meta_path

"""## Evaluation on any split (train/test)"""

def _w2v_for_meta(meta):
    return load_pickle(f"{DATA_ROOT}/{meta['size']}/{meta['dataset']}/word2vec.p")

def _xy_for_meta(meta, split="test"):
    assert split in ("train","test")
    isz = int(meta["input_size"]); dsz = int(meta["word2vec_len"])
    file_path = meta["train_file"] if split=="train" else meta["test_file"]
    w2v = _w2v_for_meta(meta)
    X, Y = get_x_y(file_path, num_classes=2, word2vec_len=dsz, input_size=isz, word2vec=w2v)
    return X, Y, file_path

def eval_from_meta_on(meta, split="test"):
    """
    Evaluate saved model on 'train' or 'test' using the model's own shapes/files.
    Returns: (metrics_dict, confusion_matrix_ndarray, curves_dict)
    """
    # model path (new or legacy)
    model_path = meta.get("paths", {}).get("model", meta.get("meta_path","").replace(".json",".keras"))
    assert os.path.exists(model_path), f"Model not found: {model_path}"

    X, Y, file_used = _xy_for_meta(meta, split=split)
    model = load_model(model_path)
    y_prob = model.predict(X, verbose=0)

    metrics = compute_all_metrics(Y, y_prob)
    curves = {
        "roc": metrics["roc_curve"],   # dict with fpr,tpr,thresholds
        "pr":  metrics["pr_curve"]     # dict with precision,recall,thresholds
    }
    return metrics, metrics["cm"], curves, file_used

# example: train baseline CNN on 1_tiny
size = "1_tiny"
size_dir = f"{DATA_ROOT}/{size}/{dataset_name}"
train_file = f"{size_dir}/train_orig.txt"

metrics, model_path, meta_path = train_and_eval(
    size=size,
    variant_label='baseline',
    train_file=train_file,
    test_file=TEST_PATH,
    model_type='cnn'
)

# re-evaluate on TEST (should match meta['test_metrics'])
meta = json.load(open(meta_path))
meta["meta_path"] = meta_path
te_metrics, te_cm, te_curves, te_file = eval_from_meta_on(meta, split="test")
print("Re-eval (TEST):", {k: v for k, v in te_metrics.items() if k not in ["cm","roc_curve","pr_curve"]})

# evaluate on TRAIN (fit quality, for diagnostics)
tr_metrics, tr_cm, tr_curves, tr_file = eval_from_meta_on(meta, split="train")
print("Re-eval (TRAIN):", {k: v for k, v in tr_metrics.items() if k not in ["cm","roc_curve","pr_curve"]})

# Toggle models
RUN_CNN  = True
RUN_LSTM = True

# Dataset sizes & EDA settings
RUN_SIZES   = ['1_tiny','2_small','3_standard','4_full']
RUN_METHODS = ['sr','ri','rd','rs']
RUN_ALPHAS  = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]   # trim if you want faster runs

print("Planned runs:")
print(" CNN :", RUN_CNN, " LSTM:", RUN_LSTM)
print(" Sizes:", RUN_SIZES)
print(" Methods:", RUN_METHODS)
print(" Alphas:", RUN_ALPHAS)

def run_full_grid(model_type):
    assert model_type in ("cnn","lstm")
    for size in RUN_SIZES:
        size_dir   = f"{DATA_ROOT}/{size}/{dataset_name}"
        train_orig = f"{size_dir}/train_orig.txt"

        # Baseline
        if not os.path.exists(train_orig):
            print(f"⚠️ Missing baseline train file: {train_orig}")
        else:
            train_and_eval(size=size, variant_label='baseline',
                           train_file=train_orig, test_file=TEST_PATH,
                           model_type=model_type)

        # EDA variants
        for method in RUN_METHODS:
            for alpha in RUN_ALPHAS:
                aug_path = f"{size_dir}/train_{method}_{alpha}.txt"
                if not os.path.exists(aug_path):
                    print(f"⚠️ Missing augmented file (skip): {aug_path}")
                    continue
                train_and_eval(size=size, variant_label='eda',
                               train_file=aug_path, test_file=TEST_PATH,
                               model_type=model_type, method=method, alpha=alpha)

if RUN_CNN:
    print("=== Running CNN grid ===")
    run_full_grid("cnn")

if RUN_LSTM:
    print("=== Running LSTM grid ===")
    run_full_grid("lstm")

print("✅ All scheduled runs completed.")

import pandas as pd

def load_all_meta_compat(models_dir=MODELS_DIR):
    rows = []
    for j in glob.glob(os.path.join(models_dir, "*.json")):
      # skip history files
        if j.endswith(".history.json") or j.endswith(".cm.json"):
            continue
        try:
            m = json.load(open(j))
            m["meta_path"]  = j
            m["_meta_path"] = j  # for legacy fallback if needed
            # normalize model path
            m["model_path"] = m.get("paths", {}).get("model", j.replace(".json",".keras"))
            rows.append(m)
        except Exception as e:
            print("Skip bad meta:", j, e)
    return pd.DataFrame(rows)

def flatten_test_metrics(meta):
    tm = meta.get("test_metrics", {})
    # cm is stored as file and not in test_metrics; we read the cm json if present
    cm_path = meta.get("paths", {}).get("cm")
    cm_list = None
    if cm_path and os.path.exists(cm_path):
        try:
            cm_list = json.load(open(cm_path))["confusion_matrix"]
        except Exception:
            cm_list = None
    return {
        "test_acc": tm.get("acc"),
        "test_precision": tm.get("precision"),
        "test_recall": tm.get("recall"),
        "test_f1": tm.get("f1"),
        "test_auroc": tm.get("auroc"),
        "test_pr_auc": tm.get("pr_auc"),
        "test_cm": cm_list
    }

# Build the table
all_rows = []
df_meta = load_all_meta_compat()
print("Found metadata files:", len(df_meta))

for _, r in df_meta.iterrows():
    meta = json.load(open(r["meta_path"]))
    meta["meta_path"] = r["meta_path"]  # ensure present for legacy logic
    meta["paths"] = r["paths"]

    # Test metrics (from meta)
    tdict = flatten_test_metrics(meta)

    # Train metrics (recomputed)
    train_metrics, train_cm, train_curves, train_file = eval_from_meta_on(meta, split="train")
    all_rows.append({
        "timestamp": meta.get("timestamp"),
        "dataset": meta.get("dataset"),
        "size": meta.get("size"),
        "model_type": meta.get("model_type"),
        "variant": meta.get("variant"),
        "method": meta.get("method"),
        "alpha": meta.get("alpha"),
        "input_size": meta.get("input_size"),
        "word2vec_len": meta.get("word2vec_len"),
        "epochs": meta.get("epochs"),
        "batch_size": meta.get("batch_size"),
        "val_split": meta.get("val_split"),
        "train_seconds": meta.get("train_seconds"),
        "train_file": meta.get("train_file"),
        "test_file": meta.get("test_file"),
        "model_path": meta.get("paths", {}).get("model", r["meta_path"].replace(".json",".keras")),
        "meta_path": r["meta_path"],

        # ---- TEST metrics (from metadata) ----
        "test_acc": tdict["test_acc"],
        "test_precision": tdict["test_precision"],
        "test_recall": tdict["test_recall"],
        "test_f1": tdict["test_f1"],
        "test_auroc": tdict["test_auroc"],
        "test_pr_auc": tdict["test_pr_auc"],
        "test_cm": tdict["test_cm"],

        # ---- TRAIN metrics (freshly computed) ----
        "train_acc": train_metrics["acc"],
        "train_precision": train_metrics["precision"],
        "train_recall": train_metrics["recall"],
        "train_f1": train_metrics["f1"],
        "train_auroc": train_metrics["auroc"],
        "train_pr_auc": train_metrics["pr_auc"],
        "train_cm": train_cm.tolist(),
    })

all_runs_df = pd.DataFrame(all_rows).sort_values(
    ["model_type","size","variant","method","alpha","timestamp"]
).reset_index(drop=True)

display(all_runs_df.head())
out_csv = f"{LOGS_DIR}/all_runs_metrics_sst2_full.csv"
all_runs_df.to_csv(out_csv, index=False)
print("✅ Saved all runs metrics to:", out_csv)

print("Rows by model_type:")
print(all_runs_df["model_type"].value_counts(), "\n")

print("Rows by size:")
print(all_runs_df["size"].value_counts(), "\n")

print("Rows by variant:")
print(all_runs_df["variant"].value_counts(), "\n")

print("Example row:")
display(all_runs_df.iloc[:1].T)

